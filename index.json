[{"authors":["admin"],"categories":null,"content":"Liangming Pan (潘亮铭) is a Postdoctoral Scholar at the Natural Language Processing Group, University of California, Santa Barbara (UCSB), working with Prof. William Yang Wang. He obtained his Ph.D. from the National University of Singapore in Jan 2022, jointly advised by Prof. Min-Yen Kan and Prof. Tat-Seng Chua. Before joining NUS, he received Master\u0026rsquo;s degree from the School of Computer Science at Tsinghua University in June 2017, working with Prof. Juanzi Li and Prof. Jie Tang. He obtained his Bachelor\u0026rsquo;s degree from Beihang University (2010 - 2014). He was a visiting Ph.D. student from 2020 to 2021, supervised by Prof. William Yang Wang.\nHis broad research interests include knowledge bases, natural language processing, and data mining. To be specific, his research topics include Question Answering, Question Generation, and Automated Fact Checking.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://teacherpeterpan.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Liangming Pan (潘亮铭) is a Postdoctoral Scholar at the Natural Language Processing Group, University of California, Santa Barbara (UCSB), working with Prof. William Yang Wang. He obtained his Ph.D. from the National University of Singapore in Jan 2022, jointly advised by Prof. Min-Yen Kan and Prof. Tat-Seng Chua. Before joining NUS, he received Master\u0026rsquo;s degree from the School of Computer Science at Tsinghua University in June 2017, working with Prof. Juanzi Li and Prof.","tags":null,"title":"Liangming Pan","type":"authors"},{"authors":["Liangming Pan","Alon Albalak","Xinyi Wang","William Yang Wang"],"categories":[],"content":"","date":1697075091,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1697075091,"objectID":"d5ac6b87a8647213db55eeb3b5e61f8a","permalink":"https://teacherpeterpan.github.io/publication/logic-llm/","publishdate":"2023-05-31T18:44:51-07:00","relpermalink":"/publication/logic-llm/","section":"publication","summary":"Large Language Models (LLMs) have shown human-like reasoning abilities but still struggle with complex logical problems. This paper introduces a novel framework, Logic-LM, which integrates LLMs with symbolic reasoning to improve logical problem-solving. Our method first utilizes LLMs to translate a natural language problem into a symbolic formulation. Afterward, a deterministic symbolic solver performs inference on the formulated problem. We also introduce a self-refinement stage, which utilizes the symbolic solver's error messages to revise symbolic formalizations. We demonstrate Logic-LM's effectiveness on four logical reasoning datasets: ProofWriter, PrOntoQA, FOLIO, and LogicalDeduction. Our results show significant improvement compared to LLMs alone, with an average performance boost of 62.6% over standard prompting and 23.5% over chain-of-thought prompting. Our findings suggest that Logic-LM, by combining LLMs with symbolic logic, offers a promising avenue for faithful logical reasoning.","tags":[],"title":"Logic-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning","type":"publication"},{"authors":["Yikang Pan*","Liangming Pan*","Wenhu Chen","Preslav Nakov","Min-Yen Kan","William Yang Wang (*equal contribution)"],"categories":[],"content":"","date":1696959975,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696959975,"objectID":"f5123fa57948b4ed0764bfbdb76c25d5","permalink":"https://teacherpeterpan.github.io/publication/emnlp23-risk/","publishdate":"2023-10-10T10:46:15-07:00","relpermalink":"/publication/emnlp23-risk/","section":"publication","summary":"In this paper, we comprehensively investigate the potential misuse of modern Large Language Models (LLMs) for generating credible-sounding misinformation and its subsequent impact on information-intensive applications, particularly Open-Domain Question Answering (ODQA) systems. We establish a threat model and simulate potential misuse scenarios, both unintentional and intentional, to assess the extent to which LLMs can be utilized to produce misinformation. Our study reveals that LLMs can act as effective misinformation generators, leading to a significant degradation in the performance of ODQA systems. To mitigate the harm caused by LLM-generated misinformation, we explore three defense strategies: prompting, misinformation detection, and majority voting. While initial results show promising trends for these defensive strategies, much more work needs to be done to address the challenge of misinformation pollution. Our work highlights the need for further research and interdisciplinary collaboration to address LLM-generated misinformation and to promote responsible use of LLMs.","tags":[],"title":"On the Risk of Misinformation Pollution with Large Language Models","type":"publication"},{"authors":["Xinyuan Lu*","Liangming Pan*","Qian Liu","Preslav Nakov","Min-Yen Kan (*equal contribution)"],"categories":[],"content":"","date":1696869988,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696869988,"objectID":"63ae248877657a58b2a2de1cddb03fea","permalink":"https://teacherpeterpan.github.io/publication/emnlp23-scitab/","publishdate":"2023-10-09T09:46:28-07:00","relpermalink":"/publication/emnlp23-scitab/","section":"publication","summary":"Scientific fact-checking is crucial for ensuring the accuracy, reliability, and trustworthiness of scientific claims. However, existing benchmarks are limited in terms of their claim diversity, reliance on text-based evidence, and oversimplification of scientific reasoning. To address these gaps, we introduce SCITAB, a novel dataset comprising 1,225 challenging scientific claims requiring compositional reasoning with scientific tables. The claims in SCITAB are derived from the actual scientific statements, and the evidence is presented as tables, closely mirroring real-world fact-checking scenarios. We establish benchmarks on SCITAB using state-of-the-art models, revealing its inherent difficulty and highlighting limitations in existing prompting methods. Our error analysis identifies unique challenges, including ambiguous expressions and irrelevant claims, suggesting future research directions.","tags":[],"title":"SCITAB: A Challenging Benchmark for Compositional Reasoning and Claim Verification on Scientific Tables","type":"publication"},{"authors":["Liangming Pan","Michael Saxon","Wenda Xu","Deepak Nathani","Xinyi Wang","William Yang Wang"],"categories":[],"content":"","date":1696823835,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696823835,"objectID":"e2591c79e3765c3f241cfc3e74731d87","permalink":"https://teacherpeterpan.github.io/publication/self-correct-survey/","publishdate":"2023-10-08T20:57:15-07:00","relpermalink":"/publication/self-correct-survey/","section":"publication","summary":"Large language models (LLMs) have demonstrated remarkable performance across a wide array of NLP tasks. However, their efficacy is undermined by undesired and inconsistent behaviors, including hallucination, unfaithful reasoning, and toxic content. A promising approach to rectify these flaws is self-correction, where the LLM itself is prompted or guided to fix problems in its own output. Techniques leveraging automated feedback -- either produced by the LLM itself or some external system -- are of particular interest as they are a promising way to make LLM-based solutions more practical and deployable with minimal human feedback. This paper presents a comprehensive review of this emerging class of techniques. We analyze and taxonomize a wide array of recent work utilizing these strategies, including training-time, generation-time, and post-hoc correction. We also summarize the major applications of this strategy and conclude by discussing future directions and challenges.","tags":[],"title":"Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies","type":"publication"},{"authors":["Liangming Pan","Xinyuan Lu","Min-Yen Kan","Preslav Nakov"],"categories":[],"content":"","date":1696700874,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696700874,"objectID":"bde013763f7c815e12035b2e1395e603","permalink":"https://teacherpeterpan.github.io/publication/emnlp23-demo/","publishdate":"2023-10-07T10:47:54-07:00","relpermalink":"/publication/emnlp23-demo/","section":"publication","summary":"Fact-checking real-world claims often requires complex, multi-step reasoning due to the absence of direct evidence to support or refute them. However, existing fact-checking systems often lack transparency in their decision-making, making it challenging for users to comprehend their reasoning process. To address this, we propose the Question-guided Multi-hop Fact-Checking (QACHECK) system, which guides the model’s reasoning process by asking a series of questions critical for verifying a claim. QACHECK has five key modules: a claim verifier, a question generator, a question-answering module, a QA validator, and a reasoner. Users can input a claim into QACHECK, which then predicts its veracity and provides a comprehensive report detailing its reasoning process, guided by a sequence of (question, answer) pairs. QACHECK also provides the source of evidence supporting each question, fostering a transparent, explainable, and user-friendly fact-checking process.","tags":[],"title":"QACHECK: A Demonstration System for Question-Guided Multi-Hop Fact-Checking","type":"publication"},{"authors":["Wenda Xu","Danqing Wang","Liangming Pan","Zhenqiao Song","Markus Freitag","William Yang Wang","Lei Li"],"categories":[],"content":"","date":1696610800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696610800,"objectID":"c8f141e05b72508db4b981f797a8b7bd","permalink":"https://teacherpeterpan.github.io/publication/emnlp23-instructscore/","publishdate":"2023-10-06T09:46:40-07:00","relpermalink":"/publication/emnlp23-instructscore/","section":"publication","summary":"Automatically evaluating the quality of language generation is critical. Although recent learned metrics show high correlation with human judgement, these metrics can not explain their verdict or associate the scores with defects in generated text. To address this limitation, we present InstructScore, an explainable evaluation metric for text generation. By harnessing both explicit human instruction and the implicit knowledge of GPT-4, we fine-tune a text evaluation metric based on LLaMA, producing both a score for generated text and a human readable diagnostic report. We evaluate InstructScore on a variety of generation tasks, including translation, captioning, data-to-text and commonsense generation. Experiments show that our 7B model surpasses all other unsupervised metrics, including those based on 175B GPT-3 and GPT-4. Surprisingly, our InstructScore, even without direct supervision from human-rated data, achieves performance levels on par with state-of-the-art metrics like COMET22, which were fine-tuned on human ratings.","tags":[],"title":"INSTRUCTSCORE: Explainable Text Generation Evaluation with Finegrained Feedback","type":"publication"},{"authors":["Deepak Nathani","David Wang","Liangming Pan","William Yang Wang"],"categories":[],"content":"","date":1696520793,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696520793,"objectID":"35b627a148a80f2cb5f2b0dc0fdd19a8","permalink":"https://teacherpeterpan.github.io/publication/emnlp23-maf/","publishdate":"2023-10-05T08:46:33-07:00","relpermalink":"/publication/emnlp23-maf/","section":"publication","summary":"Language Models (LMs) have shown impressive performance in various natural language tasks. However, when it comes to natural language reasoning, LMs still face challenges such as hallucination, generating incorrect intermediate reasoning steps, and making mathematical errors. Recent research has focused on enhancing LMs through self-improvement using feedback. Nevertheless, existing approaches relying on a single generic feedback source fail to address the diverse error types found in LM-generated reasoning chains. In this work, we propose Multi-Aspect Feedback, an iterative refinement framework that integrates multiple feedback modules, including frozen LMs and external tools, each focusing on a specific error category. Our experimental results demonstrate the efficacy of our approach to addressing several errors in the LM-generated reasoning chain and thus improving the overall performance of an LM in several reasoning tasks. We see an improvement of up to 20% in Mathematical Reasoning and up to 18% in Logical Entailment.","tags":[],"title":"MAF: Multi-Aspect Feedback for Improving Reasoning in Large Language Models","type":"publication"},{"authors":["Shizhe Diao","Yongyu Lei","Liangming Pan","Tianqing Fang","Wangchunshu Zhou","Sedrick Scott Keh","Min-Yen Kan","Tong Zhang"],"categories":[],"content":"","date":1696441613,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696441613,"objectID":"cc6a2cb4af910ec8d3fdbd58811044ae","permalink":"https://teacherpeterpan.github.io/publication/emnlp23-dolittle/","publishdate":"2023-10-04T10:46:53-07:00","relpermalink":"/publication/emnlp23-dolittle/","section":"publication","summary":"Improving the quality of academic writing is a meaningful but challenging task. Conventional methods of language refinement focus on narrow, specific linguistic features within isolated sentences, such as grammatical errors and improper word use. We propose a more general task, Academic Writing Formalization (AWF), to improve the overall quality of formal academic writing at the paragraph level. We formulate this language refinement task as a formal text style transfer task which transfers informal-academic text to formal-academic and contribute a large-scale non-parallel dataset, Doolittle, for this purpose. Concurrently, we apply a method named metric-oriented reinforcement learning (MORL) to two large language models (LLM) where we incorporate different levels of automatic feedback into the training process. Our experiments reveal that existing text transfer models and grammatical error correction models address certain aspects of AWF but still have a significant performance gap compared to human performance. Meanwhile, language models fine-tuned with our MORL method exhibit considerably improved performance, rivaling the latest chatbot ChatGPT, but still have a non-negligible gap compared to the ground truth formal-academic texts in Doolittle.","tags":[],"title":"Doolittle: Benchmarks and Corpora for Academic Writing Formalization","type":"publication"},{"authors":["Xiaobao Wu","Xinshuai Dong","Liangming Pan","Thong Thanh Nguyen","Anh Tuan Luu"],"categories":[],"content":"","date":1696355223,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696355223,"objectID":"655efafa39ef9752e742c41d716415fd","permalink":"https://teacherpeterpan.github.io/publication/emnlp23-topicmodel/","publishdate":"2023-10-03T10:47:03-07:00","relpermalink":"/publication/emnlp23-topicmodel/","section":"publication","summary":"Dynamic topic models capture the evolution of topics in sequential corpora, which have been applied in various tasks like trend analysis. However, existing models suffer from producing repetitive and unassociated topics that fail to reveal the evolution and hinder further applications. To address these issues, we in this paper propose the Unassociated word sampling and Contrastive Dynamic Topic Model. Instead of simply chaining topics as early work, we propose a contrastive topic evolution method that builds the similarity relations among dynamic topics. This captures the evolution of topics and also maintains the differences between them, which prevents producing repetitive topics. To avoid unassociated topics, we further propose an unassociated word sampling method that consistently excludes unassociated words from discovered topics. Experiments on benchmark datasets show our method significantly outperforms state-of-the-art baselines, capturing topic evolution with more coherent and diverse topics and showing better performance on downstream tasks.","tags":[],"title":"Dynamic Topic Modeling with Contrastive Topic Evolution and Unassociated Word Sampling","type":"publication"},{"authors":["Liangming Pan","Xiaobao Wu","Xinyuan Lu","Anh Tuan Luu","William Yang Wang","Min-Yen Kan","Preslav Nakov"],"categories":[],"content":"","date":1696336501,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696336501,"objectID":"0645070fc02344ebead7ef72e84b1a6d","permalink":"https://teacherpeterpan.github.io/publication/acl23-programfc/","publishdate":"2023-05-31T17:35:01-07:00","relpermalink":"/publication/acl23-programfc/","section":"publication","summary":"Fact-checking real-world claims often requires collecting multiple pieces of evidence and applying complex multi-step reasoning. In this paper, we present Program-Guided Fact-Checking (ProgramFC), a novel fact-checking model that decomposes complex claims into simpler sub-tasks that can be solved using a shared library of specialized functions. We first leverage the in-context learning ability of large language models to generate reasoning programs to guide the verification process. Afterward, we execute the program by delegating each sub-task to the corresponding sub-task handler. This process makes our model both explanatory and data-efficient, providing clear explanations of its reasoning process and requiring minimal training data. We evaluate ProgramFC on two challenging fact-checking datasets and show that it outperforms seven fact-checking baselines across different settings of evidence availability, with explicit output programs that benefit human debugging.","tags":[],"title":"Fact-Checking Complex Claims with Program-Guided Reasoning","type":"publication"},{"authors":["Liangming Pan","Wenhu Chen","Min-Yen Kan","William Yang Wang"],"categories":[],"content":"","date":1696268954,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696268954,"objectID":"d7ca83bd0b7ebaa4a7d13260eab14aa5","permalink":"https://teacherpeterpan.github.io/publication/aacl-contraqa/","publishdate":"2023-10-02T10:49:14-07:00","relpermalink":"/publication/aacl-contraqa/","section":"publication","summary":"With a rise in false, inaccurate, and misleading information in propaganda, news, and social media, real-world Question Answering (QA) systems face the challenges of synthesizing and reasoning over misinformation-polluted contexts to derive correct answers. This urgency gives rise to the need to make QA systems robust to misinformation, a topic previously unexplored. We study the risk of misinformation to QA models by investigating the sensitivity of open-domain QA models to corpus pollution with misinformation documents. We curate both human-written and model-generated false documents that we inject into the evidence corpus of QA models and assess the impact on the performance of these systems. Experiments show that QA models are vulnerable to even small amounts of evidence contamination brought by misinformation, with large absolute performance drops on all models. Misinformation attack brings more threat when fake documents are produced at scale by neural models or the attacker targets hacking specific questions of interest. To defend against such a threat, we discuss the necessity of building a misinformation-aware QA system that integrates question-answering and misinformation detection in a joint fashion.","tags":[],"title":"Attacking Open-domain Question Answering by Injecting Misinformation","type":"publication"},{"authors":["Liangming Pan","Yunxiang Zhang","Min-Yen Kan"],"categories":[],"content":"","date":1696182449,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696182449,"objectID":"76fd5d820e98080a8c4c8f18fb7569c8","permalink":"https://teacherpeterpan.github.io/publication/aacl23-dafv/","publishdate":"2023-10-01T10:47:29-07:00","relpermalink":"/publication/aacl23-dafv/","section":"publication","summary":"In this paper, we explore zero- and few-shot generalization for fact verification (FV), which aims to generalize the FV model trained on well-resourced domains (e.g., Wikipedia) to low-resourced domains that lack human annotations. To this end, we first construct a benchmark dataset collection which contains 11 FV datasets representing 6 domains. We conduct an empirical analysis of generalization across these FV datasets, finding that current models generalize poorly. Our analysis reveals that several factors affect generalization, including dataset size, length of evidence, and the type of claims. Finally, we show that two directions of work improve generalization: 1) incorporating domain knowledge via pretraining on specialized domains, and 2) automatically generating training data via claim generation.","tags":[],"title":"Investigating Zero- and Few-shot Generalization in Fact Verification","type":"publication"},{"authors":["Yan Meng*","Liangming Pan*","Yixin Cao","Min-Yen Kan (*equal contribution)"],"categories":[],"content":"","date":1696175261,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696175261,"objectID":"b4a099841636378c3b04d90b76d3ccfa","permalink":"https://teacherpeterpan.github.io/publication/aacl23-followupqg/","publishdate":"2023-10-01T08:47:41-07:00","relpermalink":"/publication/aacl23-followupqg/","section":"publication","summary":"Humans ask follow-up questions driven by curiosity, which reflects a creative human cognitive process. We introduce the task of real-world information-seeking follow-up question generation (FQG), which aims to generate follow-up questions seeking a more in-depth understanding of an initial question and answer. We construct FOLLOWUPQG, a dataset of over 3K real-world (initial question, answer, follow-up question) tuples collected from a Reddit forum providing layman-friendly explanations for open-ended questions. In contrast to existing datasets, questions in FOLLOWUPQG use more diverse pragmatic strategies to seek information, and they also show higher-order cognitive skills (such as applying and relating). We evaluate current question generation models on their efficacy for generating follow-up questions, exploring how to generate specific types of follow-up questions based on step-by-step demonstrations. Our results validate FOLLOWUPQG as a challenging benchmark, as model-generated questions are adequate but far from human-raised questions in terms of informativeness and complexity.","tags":[],"title":"FOLLOWUPQG: Towards Information-Seeking Follow-up Question Generation","type":"publication"},{"authors":[],"categories":[],"content":"","date":1691553888,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1691553888,"objectID":"f653336e1397c65d19a7094af23e707e","permalink":"https://teacherpeterpan.github.io/project/self-correct-survey/","publishdate":"2023-08-08T21:04:48-07:00","relpermalink":"/project/self-correct-survey/","section":"project","summary":"Paper list for our survey paper 'Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies'.","tags":[],"title":"Paper List for Self-Correction LLMs Survey","type":"project"},{"authors":[],"categories":[],"content":"","date":1685583740,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1685583740,"objectID":"8cf25460e5847f20001529212ea062cb","permalink":"https://teacherpeterpan.github.io/project/logic-lm/","publishdate":"2023-05-31T18:42:20-07:00","relpermalink":"/project/logic-lm/","section":"project","summary":"Codes for 'LOGIC-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning'.","tags":[],"title":"Logic-LM: Empowering LLMs with Symbolic Solvers","type":"project"},{"authors":[],"categories":[],"content":"","date":1685580576,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1685580576,"objectID":"96174d539dcffd6c1b3af3a1d3258434","permalink":"https://teacherpeterpan.github.io/project/program-fc/","publishdate":"2023-05-31T17:49:36-07:00","relpermalink":"/project/program-fc/","section":"project","summary":"Codes for ACL 2023 Paper \"Fact-Checking Complex Claims with Program-Guided Reasoning\".","tags":["Fact-Checking"],"title":"Program-FC","type":"project"},{"authors":["Shizhe Diao","Sedrick Scott Keh","Liangming Pan","Zhiliang Tian","Yan Song","Tong Zhang"],"categories":[],"content":"","date":1677387495,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677387495,"objectID":"13777a90f139a8ca745cea0749b0af00","permalink":"https://teacherpeterpan.github.io/publication/www23/","publishdate":"2023-02-25T20:58:15-08:00","relpermalink":"/publication/www23/","section":"publication","summary":"Social media classification tasks (e.g., tweet sentiment analysis, tweet stance detection) are challenging because social media posts are typically short, informal, and ambiguous. Thus, training on tweets is challenging and demands large-scale human-annotated labels, which are time-consuming and costly to obtain. In this paper, we find that providing hashtags to social media tweets can help alleviate this issue because hashtags can enrich short and ambiguous tweets in terms of various information, such as topic, sentiment, and stance. This motivates us to propose a novel Hashtag-guided Tweet Classification model (HashTation), which automatically generates meaningful hashtags for the input tweet to provide useful auxiliary signals for tweet classification. To generate high-quality and insightful hashtags, our hashtag generation model retrieves and encodes the post-level and entity-level information across the whole corpus. Experiments show that HashTation achieves significant improvements on seven low-resource tweet classification tasks, in which only a limited amount of training data is provided, showing that automatically enriching tweets with model-generated hashtags could significantly reduce the demand for large-scale human-labeled data. Further analysis demonstrates that HashTation is able to generate high-quality hashtags that are consistent with the tweets and their labels.","tags":[],"title":"Hashtag-Guided Low-Resource Tweet Classification","type":"publication"},{"authors":["Xuan Long Do","Bowei Zou","Liangming Pan","Nancy F Chen","Shafiq Joty","Ai Ti Aw"],"categories":[],"content":"","date":1660625776,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1660625776,"objectID":"4fa11f651eab61f16bccbda2acef7491","permalink":"https://teacherpeterpan.github.io/publication/coling22-do/","publishdate":"2022-08-15T20:56:16-08:00","relpermalink":"/publication/coling22-do/","section":"publication","summary":"Conversational question generation (CQG) serves as a vital task for machines to assist humans, such as interactive reading comprehension, through conversations. Compared to traditional single-turn question generation (SQG), CQG is more challenging in the sense that the generated question is required not only to be meaningful, but also to align with the occurred conversation history. While previous studies mainly focus on how to model the flow and alignment of the conversation, there has been no thorough study to date on which parts of the context and history are necessary for the model. We argue that shortening the context and history is crucial as it can help the model to optimise more on the conversational alignment property. To this end, we propose CoHS-CQG, a two-stage CQG framework, which adopts a CoHS module to shorten the context and history of the input. In particular, CoHS selects contiguous sentences and history turns according to their relevance scores by a top-p strategy. Our model achieves state-of-the-art performances on CoQA in both the answer-aware and answer-unaware settings.","tags":[],"title":"CoHS-CQG: Context and History Selection for Conversational Question Generation","type":"publication"},{"authors":["Huanli Gong","Liangming Pan","Hengchang Hu"],"categories":[],"content":"","date":1660539347,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1660539347,"objectID":"acbe05dd99420994e58a06e20569f839","permalink":"https://teacherpeterpan.github.io/publication/coling22-khan/","publishdate":"2022-08-14T20:55:47-08:00","relpermalink":"/publication/coling22-khan/","section":"publication","summary":"Designing in-depth educational questions is a time-consuming and cognitively demanding task. Therefore, it is intriguing to study how to build Question Generation (QG) models to automate the question creation process. However, existing QG datasets are not suitable for educational question generation because the questions are not real questions asked by humans during learning and can be solved by simply searching for information. To bridge this gap, we present KHANQ, a challenging dataset for educational question generation, containing 1,034 high-quality learner-generated questions seeking an in-depth understanding of the taught online courses in Khan Academy. Each data sample is carefully paraphrased and annotated as a triple of 1) Context: an independent paragraph on which the question is based; 2) Prompt: a text prompt for the question (eg, the learner’s background knowledge); 3) Question: a deep question based on Context and coherent with Prompt. By conducting a human evaluation on the aspects of appropriateness, coverage, coherence, and complexity, we show that state-of-the-art QG models which perform well on shallow question generation datasets have difficulty in generating useful educational questions. This makes KHANQ a challenging testbed for educational question generation.","tags":[],"title":"KHANQ: A Dataset for Generating Deep Questions in Education","type":"publication"},{"authors":["Shulin Cao","Jiaxin Shi","Liangming Pan","Lunyiu Nie","Yutong Xiang","Lei Hou","Juanzi Li","Hanwang Zhang","Bin He"],"categories":[],"content":"","date":1646705579,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646705579,"objectID":"45163b9613090ed2db29108ab7207dd9","permalink":"https://teacherpeterpan.github.io/publication/acl22-kqapro/","publishdate":"2022-03-08T10:12:59+08:00","relpermalink":"/publication/acl22-kqapro/","section":"publication","summary":"Complex question answering over knowledge base (Complex KBQA) is challenging because it requires various compositional reasoning capabilities, such as multi-hop inference, attribute comparison, set operation, etc.  Existing benchmarks have some shortcomings that limit the development of Complex KBQA: 1) they only provide QA pairs without explicit reasoning processes; 2) questions are poor in diversity or scale. To this end, we introduce KQA Pro, a dataset for Complex KBQA including around 120K diverse natural language questions. We introduce a compositional and interpretable programming language KoPL to represent the reasoning process of complex questions. For each question, we provide the corresponding KoPL program and SPARQL query, so that KQA Pro can serve for both KBQA and semantic parsing tasks. Experimental results show that state-of-the-art KBQA methods cannot achieve promising results on KQA Pro as on current datasets, which suggests that KQA Pro is challenging and Complex KBQA requires further research efforts. We also treat KQA Pro as a diagnostic dataset for testing multiple reasoning skills, conduct a thorough evaluation of existing models and discuss further directions for Complex KBQA.","tags":[],"title":"KQA Pro: A Dataset with Explicit Compositional Programs for Complex Question Answering over Knowledge Base","type":"publication"},{"authors":["Yunxiang Zhang","Liangming Pan","Samson Tan","Min-Yen Kan"],"categories":[],"content":"","date":1646620806,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646620806,"objectID":"1a97510d19ce5bd3bb6129773f619607","permalink":"https://teacherpeterpan.github.io/publication/acl22-causal/","publishdate":"2022-03-07T10:40:06+08:00","relpermalink":"/publication/acl22-causal/","section":"publication","summary":"Modern Natural Language Processing (NLP) models are known to be sensitive to input perturbations and their performance can decrease when applied to real-world, noisy data.  However, it is still unclear why models are less robust to some perturbations than others. In this work, we test the hypothesis that the extent to which a model is affected by an unseen textual perturbation (robustness) can be explained by the learnability of the perturbation (defined as how well the model learns to identify the perturbation with a small amount of evidence). We further give a causal justification for the learnability metric. We conduct extensive experiments with four prominent NLP models --- TextRNN, BERT, RoBERTa and XLNet --- over eight types of textual perturbations on three datasets. We show that a model which is better at identifying a perturbation (higher learnability) becomes worse at ignoring such a perturbation at test time (lower robustness), providing empirical support for our hypothesis.","tags":[],"title":"Interpreting the Robustness of Neural NLP Models to Textual Perturbations","type":"publication"},{"authors":[],"categories":[],"content":"","date":1623546470,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623546470,"objectID":"0d4e40311aae4ce6167af268b0381d37","permalink":"https://teacherpeterpan.github.io/project/qacg/","publishdate":"2021-06-13T09:07:50+08:00","relpermalink":"/project/qacg/","section":"project","summary":"Codes for ACL-IJCNLP 2021 Paper \"Zero-shot Fact Verification by Claim Generation\". ","tags":["Question Generation","Claim Generation","Fact Verification"],"title":"Zero-shot-Fact-Verification","type":"project"},{"authors":["Liangming Pan","Wenhu Chen","Wenhan Xiong","Min-Yen Kan","William Yang Wang"],"categories":[],"content":"","date":1622242729,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622242729,"objectID":"15b52844c28115e876f2c4a2938a850d","permalink":"https://teacherpeterpan.github.io/publication/acl21/","publishdate":"2021-05-29T06:58:49+08:00","relpermalink":"/publication/acl21/","section":"publication","summary":"Neural models for automated fact verification have achieved promising results thanks to the availability of large, human-annotated datasets. However, for each new domain that requires fact verification, creating a dataset by manually writing claims and linking them to their supporting evidence is expensive. We develop QACG, a framework for training a robust fact verification model by using automatically-generated claims that can be supported, refuted, or not verifiable from evidence from Wikipedia. QACG generates question-answer pairs from the evidence and then convert them into different types of claims. Experiments on the FEVER dataset show QACG framework significantly reduces the demand for human-annotated training data. On a zero-shot scenario, QACG improves a RoBERTa model's F1 from 50% to 77%, equivalent performance to over 2K manually-curated examples. ","tags":[],"title":"Zero-shot Fact Verification by Claim Generation","type":"publication"},{"authors":[],"categories":[],"content":"","date":1617058546,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617058546,"objectID":"e728be6bdab9997469e1b861f8d5b8a4","permalink":"https://teacherpeterpan.github.io/project/mqa-qg/","publishdate":"2021-03-29T15:55:46-07:00","relpermalink":"/project/mqa-qg/","section":"project","summary":"Codes for NAACL 2021 Paper \"Unsupervised Multi-hop Question Answering by Question Generation\". ","tags":["Question Generation"],"title":"Unsupervised-Multi-hop-QA","type":"project"},{"authors":["Liangming Pan","Wenhu Chen","Wenhan Xiong","Min-Yen Kan","William Yang Wang"],"categories":[],"content":"","date":1615442141,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615442141,"objectID":"118a4e142baa2b5973c1d51317bd79dd","permalink":"https://teacherpeterpan.github.io/publication/naacl21/","publishdate":"2021-03-10T21:55:41-08:00","relpermalink":"/publication/naacl21/","section":"publication","summary":"Obtaining training data for multi-hop question answering (QA) is time-consuming and resource-intensive. We explore the possibility to train a well-performed multi-hop QA model without referencing any human-labeled multi-hop question-answer pairs, i.e., unsupervised multi-hop QA. We propose MQA-QG, an unsupervised framework that can generate human-like multi-hop training data from both homogeneous and heterogeneous data sources. MQA-QG generates questions by first selecting/generating relevant information from each data source and then integrating the multiple information to form a multi-hop question. Using only generated training data, we can train a competent multi-hop QA which achieves 61% and 83% of the supervised learning performance for the HybridQA and the HotpotQA dataset, respectively. We also show that pretraining the QA system with the generated data would greatly reduce the demand for human-annotated training data. ","tags":[],"title":"Unsupervised Multi-hop Question Answering by Question Generation","type":"publication"},{"authors":["Yuxi Xie","Liangming Pan","Dongzhe Wang","Min-Yen Kan","Yansong Feng"],"categories":[],"content":"","date":1604727369,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604727369,"objectID":"269cbf5647448091877e39604fff6426","permalink":"https://teacherpeterpan.github.io/publication/coling20/","publishdate":"2020-11-06T21:36:09-08:00","relpermalink":"/publication/coling20/","section":"publication","summary":"Recent question generation (QG) approaches often utilize the sequence-to-sequence framework (Seq2Seq) to optimize the log-likelihood of ground-truth questions using teacher forcing. However, this training objective is inconsistent with actual question quality, which is often reflected by certain global properties such as whether the question can be answered by the document. As such, we directly optimize for QG-specific objectives via reinforcement learning to improve question quality. We design three different rewards that target to improve the fluency, relevance, and answerability of generated questions. We conduct both automatic and human evaluations in addition to a thorough analysis to explore the effect of each QG-specific reward. We find that optimizing question-specific rewards generally leads to better performance in automatic evaluation metrics. However, only the rewards that correlate well with human judgement (e.g., relevance) lead to real improvement in question quality. Optimizing for the others, especially answerability, introduces incorrect bias to the model, resulting in poor question quality.","tags":[],"title":"Exploring Question-Specific Rewards for Generating Deep Questions","type":"publication"},{"authors":["Zhiyuan Liu","Yixin Cao","Liangming Pan","Juanzi Li","Zhiyuan Liu","Tat-Seng Chua"],"categories":[],"content":"","date":1602566692,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602566692,"objectID":"63d78a51cf541d043d288d7dd8f8ce45","permalink":"https://teacherpeterpan.github.io/publication/emnlp20/","publishdate":"2020-10-12T22:24:52-07:00","relpermalink":"/publication/emnlp20/","section":"publication","summary":"Entity alignment (EA) aims at building a unified Knowledge Graph (KG) of rich content by linking the equivalent entities from various KGs. GNN-based EA methods present promising performances by modeling the KG structure defined by relation triples. However, attribute triples can also provide crucial alignment signal but have not been well explored yet. In this paper, we propose to utilize an attributed value encoder and partition the KG into subgraphs to model the various types of attribute triples efficiently. Besides, the performances of current EA methods are overestimated because of the name-bias of existing EA datasets. To make an objective evaluation, we propose a hard experimental setting where we select equivalent entity pairs with very different names as the test set. Under both the regular and hard settings, our method achieves significant improvements (5.10% on average Hits@1 in DBP15k) over 12 baselines in cross-lingual and monolingual datasets. Ablation studies on different subgraphs and a case study about attribute types further demonstrate the effectiveness of our method. Source code and data can be found at this https URL.","tags":[],"title":"Exploring and Evaluating Attributes, Values, and Structures for Entity Alignment","type":"publication"},{"authors":["Liangming Pan","Jingjing Chen","Jianlong Wu","Shaoteng Liu","Chong-Wah Ngo","Min-Yen Kan","Yu-Gang Jiang","Tat-Seng Chua"],"categories":[],"content":"","date":1597551852,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597551852,"objectID":"6e7bc11a87ac468c7ba73c3f76c64923","permalink":"https://teacherpeterpan.github.io/publication/acmmm-2020/","publishdate":"2020-08-15T21:24:12-07:00","relpermalink":"/publication/acmmm-2020/","section":"publication","summary":"Understanding food recipe requires anticipating the implicit causal effects of cooking actions, such that the recipe can be converted into a graph describing the temporal workflow of the recipe. This is a non-trivial task that involves common-sense reasoning. However, existing efforts rely on hand-crafted features to extract the workflow graph from recipes due to the lack of large-scale labeled datasets. Moreover, they fail to utilize the cooking images, which constitute an important part of food recipes. In this paper, we build MM-ReS, the first large-scale dataset for cooking workflow construction, consisting of 9,850 recipes with human-labeled workflow graphs. Cooking steps are multi-modal, featuring both text instructions and cooking images. We then propose a neural encoder-decoder model that utilizes both visual and textual information to construct the cooking workflow, which achieved over 20% performance gain over existing hand-crafted baselines. ","tags":[],"title":"Multi-modal Cooking Workflow Construction for Food Recipes","type":"publication"},{"authors":["Liangming Pan","Jingjing Chen","Shaoteng Liu","Chong-Wah Ngo","Min-Yen Kan","Tat-Seng Chua"],"categories":[],"content":"","date":1591102883,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591102883,"objectID":"bc29564a513a9e32abf7e46f8e841e9b","permalink":"https://teacherpeterpan.github.io/publication/tmm20/","publishdate":"2020-06-02T21:01:23+08:00","relpermalink":"/publication/tmm20/","section":"publication","summary":"Modeling the structure of culinary recipes is the core of recipe representation learning. Current approaches mostly focus on extracting the workflow graph from recipes based on text descriptions. Process images, which constitute an important part of cooking recipes, has rarely been investigated in recipe structure modeling. We study this recipe structure problem from a multi-modal learning perspective, by proposing a prerequisite tree to represent recipes with cooking images at a step-level gran-ularity. We propose a simple-yet-effective two-stage framework to automatically construct the prerequisite tree for a recipe by (1) utilizing a trained classifier to detect pairwise prerequisite relations that fuses multi-modal features as input; then (2) applying different strategies (greedy method, maximum weight, and beam search) to build the tree structure. Experiments on the MM-ReS dataset demonstrates the advantages of introducing process images for recipe structure modeling. Also, compared with neural methods which require large numbers of training data, we show that our two-stage pipeline can achieve promising results using only 400 labeled prerequisite trees as training data.","tags":[],"title":"A Hybrid Approach for Detecting Prerequisite Relations in Multi-modal Food Recipes","type":"publication"},{"authors":[],"categories":[],"content":"","date":1588208161,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588208161,"objectID":"402255db29d355cdf1c944fee248d3b8","permalink":"https://teacherpeterpan.github.io/project/sg-dqg/","publishdate":"2020-04-30T08:56:01+08:00","relpermalink":"/project/sg-dqg/","section":"project","summary":"This repository contains code and models for the paper: Semantic Graphs for Generating Deep Questions (ACL 2020). ","tags":["Question Generation"],"title":"SG-Deep Question Generation","type":"project"},{"authors":["Liangming Pan","Yuxi Xie","Yansong Feng","Tat-Seng Chua","Min-Yen Kan"],"categories":[],"content":"","date":1588046727,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588046727,"objectID":"f8f68b2e45098059e8d058a6547385f0","permalink":"https://teacherpeterpan.github.io/publication/acl20-qg/","publishdate":"2020-04-28T12:05:27+08:00","relpermalink":"/publication/acl20-qg/","section":"publication","summary":"This paper proposes the problem of Deep Question Generation (DQG), which aims to generate complex questions that require reasoning over multiple pieces of information of the input passage. In order to capture the global structure of the document and facilitate reasoning, we propose a novel framework which first constructs a semantic-level graph for the input document and then encodes the semantic graph by introducing an attention-based GGNN (Att-GGNN). Afterwards, we fuse the document-level and graph-level representations to perform joint training of content selection and question decoding. On the HotpotQA deep-question centric dataset, our model greatly improves performance over questions requiring reasoning over multiple facts, leading to state-of-the-art performance. The code is publicly available at https://github.com/WING-NUS/SG-Deep-Question-Generation.","tags":[],"title":"Semantic Graphs for Generating Deep Questions","type":"publication"},{"authors":["Yixin Cao","Ruihao Shui","Liangming Pan","Min-Yen Kan","Zhiyuan Liu","Tat-Seng Chua"],"categories":[],"content":"","date":1587950204,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587950204,"objectID":"19091f232f7e9ea21f4ad11d26701470","permalink":"https://teacherpeterpan.github.io/publication/acl20-style/","publishdate":"2020-04-27T09:16:44+08:00","relpermalink":"/publication/acl20-style/","section":"publication","summary":"The curse of knowledge can impede communication between experts and laymen. We propose a new task of expertise style transfer and contribute a manually annotated dataset with the goal of alleviating such cognitive biases. Solving this task not only simplifies the professional language, but also improves the accuracy and expertise level of laymen descriptions using simple words. This is a challenging task, unaddressed in previous work, as it requires the models to have expert intelligence in order to modify text with a deep understanding of domain knowledge and structures. We establish the benchmark performance of five state-of-the-art models for style transfer and text simplification. The results demonstrate a significant gap between machine and human performance. We also discuss the challenges of automatic evaluation, to provide insights into future research directions. The dataset is publicly available at https://srhthu.github.io/expertise-style-transfer/.","tags":[],"title":"Expertise Style Transfer: A New Task Towards Better Communcation between Experts and Laymen","type":"publication"},{"authors":["Shaoteng Liu","Jingjing Chen","Liangming Pan","Chong-Wah Ngo","Tat-Seng Chua","Yu-Gang Jiang"],"categories":[],"content":"","date":1587777416,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587777416,"objectID":"7777de6db028b828f9af5e10f31772f9","permalink":"https://teacherpeterpan.github.io/publication/cvpr20/","publishdate":"2020-04-25T09:16:56+08:00","relpermalink":"/publication/cvpr20/","section":"publication","summary":"This paper proposes a Hyperbolic Visual Embedding Learning Network for zero-shot recognition. The network learns image embeddings in hyperbolic space, which is capable of preserving the hierarchical structure of semantic classes in low dimensions. Comparing with existing zero-shot learning approaches, the network is more robust because the embedding feature in hyperbolic space better represents class hierarchy and thereby avoid misleading resulted from unrelated siblings. Our network outperforms exiting baselines under hierarchical evaluation with an extremely challenging setting, i.e., learning only from 1,000 categories to recognize 20,841 unseen categories. While under flat evaluation, it has competitive performance as state-of-the-art methods but with five times lower embedding dimensions. Our code is publicly available.","tags":[],"title":"Hyperbolic Visual Embedding Learning for Zero-Shot Recognition","type":"publication"},{"authors":["Jingjing Chen","Liangming Pan","Zhipeng Wei","Xiang Wang","Chong-Wah Ngo","Tat-Seng Chua"],"categories":[],"content":"","date":1579275928,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579275928,"objectID":"b9df0089196231b9165749e5efeeb3aa","permalink":"https://teacherpeterpan.github.io/publication/aaai-2020/","publishdate":"2020-01-17T23:45:28+08:00","relpermalink":"/publication/aaai-2020/","section":"publication","summary":"Recognizing ingredients for a given dish image is at the core of automatic dietary assessment, attracting increasing attention from both industry and academia. Nevertheless, the task is challenging due to the difficulty of collecting and labeling sufficient training data. On one hand, there are hundred thousands of food ingredients in the world, ranging from the common to rare. Collecting training samples for all of the ingredient categories is difficult. On the other hand, as the ingredient appearances exhibit huge visual variance during the food preparation, it requires to collect the training samples under different cooking and cutting methods for robust recognition. Since obtaining sufficient fully annotated training data is not easy, a more practical way of scaling up the recognition is to develop models that are capable of recognizing unseen ingredients. Therefore, in this paper, we target the problem of ingredient recognition with zero training samples. More specifically, we introduce multi-relational GCN (graph convolutional network) that integrates ingredient hirerarchy, attribute as well as co-occurrence for zero-shot ingredient recognition. Extensive experiments on both Chinese and Japanese food datasets are performed to demonstrate the superior performance of multi-relational GCN and shed light on zero-shot ingredients recognition.","tags":[],"title":"Zero-shot Ingredient Recognition by Multi-Relational Graph Convolutional Network","type":"publication"},{"authors":null,"categories":null,"content":"","date":1569542400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569542400,"objectID":"adb0b4493abbb66665061a12c910d5dc","permalink":"https://teacherpeterpan.github.io/project/question-generation/","publishdate":"2019-09-27T00:00:00Z","relpermalink":"/project/question-generation/","section":"project","summary":"A paper list that summarizes the recent papers on neural question generation.","tags":["Question Generation"],"title":"Question Generation Paper List","type":"project"},{"authors":["Liangming Pan","Wenqiang Lei","Tat-Seng Chua","Min-Yan Kan"],"categories":[],"content":"","date":1558490102,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558490102,"objectID":"324bd0be88c04577bede7112b65338ef","permalink":"https://teacherpeterpan.github.io/publication/qg-survey/","publishdate":"2019-05-22T09:55:02+08:00","relpermalink":"/publication/qg-survey/","section":"publication","summary":"Emerging research in Neural Question Generation (NQG) has started to integrate a larger variety of inputs, and generating questions requiring higher levels of cognition. These trends point to NQG as a bellwether for NLP, about how human intelligence embodies the skills of curiosity and integration. We present a comprehensive survey of neural question generation, examining the corpora, methodologies, and evaluation methods. From this, we elaborate on what we see as emerging on NQG's trend: in terms of the learning paradigms, input modalities, and cognitive levels considered by NQG. We end by pointing out the potential directions ahead.","tags":[],"title":"Recent Advances in Neural Question Generation","type":"publication"},{"authors":["Yahui An","Liangming Pan","Min-Yen Kan","Qiang Dong","Yan Fu"],"categories":[],"content":"","date":1551318970,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551318970,"objectID":"eb2011c51d47589190fae2e8a9e4b51d","permalink":"https://teacherpeterpan.github.io/publication/ieee-access/","publishdate":"2019-02-28T09:56:10+08:00","relpermalink":"/publication/ieee-access/","section":"publication","summary":"In discussions hosted on discussion forums for massive online open courses (MOOCs), references to online learning resources are often of central importance. They contextualize the discussion, anchoring the discussion participants' presentation of the issues and their understanding. However, they are usually mentioned in free text, without appropriate hyperlinking to their associated resource. Automated learning resource mention hyperlinking and categorization will facilitate discussion and searching within the MOOC forums and also benefit the contextualization of such resources across disparate views. We propose the novel problem of learning resource mention identification in MOOC forums, i.e., to identify resource mentions in discussions and classify them into pre-defined resource types. As this is a novel task with no publicly available data, we first contribute a large-scale labeled dataset-dubbed the forum resource mention (FoRM) dataset-to facilitate our current research and future research on this task. The FoRM contains over 10 000 real-world forum threads in collaboration with Coursera, with more than 23 000 manually labeled resource mentions. We then formulate this task as a sequence tagging problem and investigate solution architectures to address the problem. Importantly, we identify two major challenges that hinder the applications of sequence tagging models to the task: (1) the diversity of resource mention expression and (2) long-range contextual dependencies. We address these challenges by incorporating character-level and thread context information into an LSTM-CRF model. First, we incorporate a character encoder to address the out-of-vocabulary problem caused by the diversity of mention expressions. Second, to address the context dependency challenge, we encode thread contexts using an RNN-based context encoder and apply the attention mechanism to selectively leverage useful context information during sequence tagging. The experiments on FoRM show that the proposed method improves the baseline deep sequence tagging models notably, significantly bettering performance on instances that exemplify two challenges.","tags":[],"title":"Resource Mention Extraction for MOOC Discussion Forums","type":"publication"},{"authors":["Jifan Yu","Liangming Pan","Juanzi Li","Xiaoping Du"],"categories":[],"content":"","date":1533088584,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1533088584,"objectID":"36ea01f4c1e1150ccd81a173616b38c9","permalink":"https://teacherpeterpan.github.io/publication/ccks18/","publishdate":"2018-08-01T09:56:24+08:00","relpermalink":"/publication/ccks18/","section":"publication","summary":"Applying data mining techniques to help researchers discover, understand, and predict research trends is a highly beneficial but challenging task. The existing researches mainly use topics extracted from literatures as objects to build predicting model. To get more accurate results, we use concepts instead of topics constructing a model to predict their rise and fall trends, considering the rhetorical characteristics of them. The experimental results based on ACL1965-2017 literature dataset show the clues of the scientific trends can be found in the rhetorical distribution of concepts. After adding the relevant concepts’ information, the predict model’s accuracy rate can be significantly improved, compared to the prior topic-based algorithm. ","tags":[],"title":"Predicting Concept-based Research Trends with Rhetorical Framing","type":"publication"},{"authors":["Liangming Pan","Xiaochen Wang","Chengjiang Li","Juanzi Li","Jie Tang"],"categories":[],"content":"","date":1512093378,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512093378,"objectID":"d1df4bd15b37da05986c0159c7c5bd72","permalink":"https://teacherpeterpan.github.io/publication/ijcnlp17/","publishdate":"2017-12-01T09:56:18+08:00","relpermalink":"/publication/ijcnlp17/","section":"publication","summary":"Massive Open Online Courses (MOOCs), offering a new way to study online, are revolutionizing education. One challenging issue in MOOCs is how to design effective and fine-grained course concepts such that students with different backgrounds can grasp the essence of the course. In this paper, we conduct a systematic investigation of the problem of course concept extraction for MOOCs. We propose to learn latent representations for candidate concepts via an embedding-based method. Moreover, we develop a graph-based propagation algorithm to rank the candidate concepts based on the learned representations. We evaluate the proposed method using different courses from XuetangX and Coursera. Experimental results show that our method significantly outperforms all the alternative methods (+0.013-0.318 in terms of R-precision; p","tags":[],"title":"Course Concept Extraction in MOOCs via Embedding-Based Graph Propagation","type":"publication"},{"authors":["Liangming Pan","Chengjiang Li","Juanzi Li","Jie Tang"],"categories":[],"content":"","date":1498874161,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498874161,"objectID":"9b2e3f61477636120fe243b19e0d3faa","permalink":"https://teacherpeterpan.github.io/publication/acl17/","publishdate":"2017-07-01T09:56:01+08:00","relpermalink":"/publication/acl17/","section":"publication","summary":"What prerequisite knowledge should students achieve a level of mastery before moving forward to learn subsequent coursewares? We study the extent to which the prerequisite relation between knowledge concepts in Massive Open Online Courses (MOOCs) can be inferred automatically. In particular, what kinds of information can be leverage to uncover the potential prerequisite relation between knowledge concepts. We first propose a representation learning-based method for learning latent representations of course concepts, and then investigate how different features capture the prerequisite relations between concepts. Our experiments on three datasets form Coursera show that the proposed method achieves significant improvements (+5.9-48.0% by F1-score) comparing with existing methods. ","tags":[],"title":"Prerequisite Relation Learning for Concepts in MOOCs","type":"publication"},{"authors":["Liangming Pan","Zhigang Wang","Juanzi Li","Jie Tang"],"categories":[],"content":"","date":1475286994,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1475286994,"objectID":"362cc5853095be919b76bd84006362ac","permalink":"https://teacherpeterpan.github.io/publication/ksem16/","publishdate":"2016-10-01T09:56:34+08:00","relpermalink":"/publication/ksem16/","section":"publication","summary":"The global knowledge sharing makes large-scale multi-lingual knowledge bases an extremely valuable resource in the Big Data era. However, current mainstream multi-lingual ontologies based on online wikis still face the limited coverage of cross-lingual knowledge links. Linking the knowledge entries distributed in different online wikis will immensely enrich the information in the online knowledge bases and benefit many applications. In this paper, we propose an unsupervised framework for cross-lingual knowledge linking. Different from traditional methods, we target the cross-lingual knowledge linking task on specific domains. We evaluate the proposed method on two knowledge linking tasks to find English-Chinese knowledge links. Experiments on English Wikipedia and Baidu Baike show that the precision improvement of cross-lingual link prediction achieve the highest 6.12 % compared with the state-of-art methods. ","tags":[],"title":"Domain Specific Cross-Lingual Knowledge Linking Based on Similarity Flooding","type":"publication"},{"authors":["Zhigang Wang","Liangming Pan","Juanzi Li","Shuangjie Li","Mingyang Li","Jie Tang"],"categories":[],"content":"","date":1473990987,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1473990987,"objectID":"e670423192cf8d097a0084cde3d666b2","permalink":"https://teacherpeterpan.github.io/publication/ccks16/","publishdate":"2016-09-16T09:56:27+08:00","relpermalink":"/publication/ccks16/","section":"publication","summary":"The global knowledge sharing makes large-scale multi-lingual knowledge bases an extremely valuable resource in the Big Data era. However, current mainstream Wikipedia-based multi-lingual ontologies still face the following problems: the scarcity of non-English knowledge, the noise in the multi-lingual ontology schema relations and the limited coverage of cross-lingual owl:sameAs relations. Building a cross-lingual ontology based on other large-scale heterogenous online wikis is a promising solution for those problems. In this paper, we propose a cross-lingually boosting approach to iteratively reinforce the performance of ontology building and instance matching. Experiments output an ontology containing over 3,520,000 English instances, 800,000 Chinese instances, and over 150,000 cross-lingual instance alignments. The F1-measure improvement of Chinese instanceOf prediction achieve the highest 32%. ","tags":[],"title":"Boosting to Build a Large-Scale Cross-Lingual Ontology","type":"publication"},{"authors":["Yan Zhang","Hailong Jin","Liangming Pan","Juanzi Li"],"categories":[],"content":"","date":1472695004,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1472695004,"objectID":"d88de22ba48158ca28980dc190a9577b","permalink":"https://teacherpeterpan.github.io/publication/oaei16/","publishdate":"2016-09-01T09:56:44+08:00","relpermalink":"/publication/oaei16/","section":"publication","summary":"This paper presents the results of RiMOM in the Ontology Alignment Evaluation Initiative (OAEI) 2016. RiMOM participated in all three tracks of Instance Matching this year. In this paper, we first describe the overall framework of our system (RiMOM). Then we detail the techniques used in the framework for instance matching. Last, we give a thorough analysis on our results and discuss some future work on RiMOM. ","tags":[],"title":"RiMOM results for OAEI 2016","type":"publication"}]